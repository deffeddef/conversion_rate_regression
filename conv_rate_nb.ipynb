{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclaimer:\n",
    "\n",
    "I am aware that most of this model's predictions will actually be negative values and although these negative predictions for practical business purposes might not be useful, they might actually lead to a better MSE when this model is being evaluated.\n",
    "\n",
    "For future references, I would actually model this problem differently. Instead aggregating on an entity level, I would aggerate the data on a conversion level, so that conversion would become our target feature and would always be either 0 or 1. Then, instead of making this a regression problem, I would approach it as a calibrated classification problem, with the output being the probability of conversion. Mind you, that for reliable results, the classification must be calibrated, so that the classification probabilities can be considered valid. Lastly, instead of MSE, a metric that measures probabilistic predictions, such as the Brier Score, might be adequate.\n",
    "\n",
    "Since the instructions for this task were quite clear, I followed them regardless of my possible objections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "import category_encoders as ce\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the csv file containing the training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id</th>\n",
       "      <th>att1</th>\n",
       "      <th>att2</th>\n",
       "      <th>att3</th>\n",
       "      <th>att4</th>\n",
       "      <th>att5</th>\n",
       "      <th>att6</th>\n",
       "      <th>att7</th>\n",
       "      <th>att8</th>\n",
       "      <th>att9</th>\n",
       "      <th>...</th>\n",
       "      <th>att154</th>\n",
       "      <th>att155</th>\n",
       "      <th>att156</th>\n",
       "      <th>att157</th>\n",
       "      <th>att158</th>\n",
       "      <th>att159</th>\n",
       "      <th>att160</th>\n",
       "      <th>week</th>\n",
       "      <th>Clicks</th>\n",
       "      <th>Conversions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>44945.000000</td>\n",
       "      <td>44945.000000</td>\n",
       "      <td>44945.000000</td>\n",
       "      <td>44945.000000</td>\n",
       "      <td>44945.000000</td>\n",
       "      <td>44945.000000</td>\n",
       "      <td>44945.000000</td>\n",
       "      <td>44945.000000</td>\n",
       "      <td>44945.000000</td>\n",
       "      <td>44945.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>44945.000000</td>\n",
       "      <td>44945.000000</td>\n",
       "      <td>44945.000000</td>\n",
       "      <td>44945.000000</td>\n",
       "      <td>44945.000000</td>\n",
       "      <td>44945.000000</td>\n",
       "      <td>44945.000000</td>\n",
       "      <td>44945.000000</td>\n",
       "      <td>44945.000000</td>\n",
       "      <td>44945.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11357.256647</td>\n",
       "      <td>0.450973</td>\n",
       "      <td>0.349694</td>\n",
       "      <td>0.921170</td>\n",
       "      <td>0.549027</td>\n",
       "      <td>0.644210</td>\n",
       "      <td>0.349694</td>\n",
       "      <td>0.391946</td>\n",
       "      <td>0.050328</td>\n",
       "      <td>0.022939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>0.002403</td>\n",
       "      <td>0.003604</td>\n",
       "      <td>0.390077</td>\n",
       "      <td>0.003204</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.003404</td>\n",
       "      <td>30.494271</td>\n",
       "      <td>8.055779</td>\n",
       "      <td>0.467727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6715.822919</td>\n",
       "      <td>0.497596</td>\n",
       "      <td>0.476879</td>\n",
       "      <td>0.269476</td>\n",
       "      <td>0.497596</td>\n",
       "      <td>0.478757</td>\n",
       "      <td>0.476879</td>\n",
       "      <td>0.488190</td>\n",
       "      <td>0.218624</td>\n",
       "      <td>0.149711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059559</td>\n",
       "      <td>0.048961</td>\n",
       "      <td>0.059929</td>\n",
       "      <td>0.487773</td>\n",
       "      <td>0.056513</td>\n",
       "      <td>0.059374</td>\n",
       "      <td>0.058246</td>\n",
       "      <td>0.499973</td>\n",
       "      <td>30.756230</td>\n",
       "      <td>1.963123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4789.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12194.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16640.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22836.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1801.000000</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          entity_id          att1          att2          att3          att4  \\\n",
       "count  44945.000000  44945.000000  44945.000000  44945.000000  44945.000000   \n",
       "mean   11357.256647      0.450973      0.349694      0.921170      0.549027   \n",
       "std     6715.822919      0.497596      0.476879      0.269476      0.497596   \n",
       "min        4.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%     4789.000000      0.000000      0.000000      1.000000      0.000000   \n",
       "50%    12194.000000      0.000000      0.000000      1.000000      1.000000   \n",
       "75%    16640.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "max    22836.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "               att5          att6          att7          att8          att9  \\\n",
       "count  44945.000000  44945.000000  44945.000000  44945.000000  44945.000000   \n",
       "mean       0.644210      0.349694      0.391946      0.050328      0.022939   \n",
       "std        0.478757      0.476879      0.488190      0.218624      0.149711   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        1.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        1.000000      1.000000      1.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       ...        att154        att155        att156        att157  \\\n",
       "count  ...  44945.000000  44945.000000  44945.000000  44945.000000   \n",
       "mean   ...      0.003560      0.002403      0.003604      0.390077   \n",
       "std    ...      0.059559      0.048961      0.059929      0.487773   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...      0.000000      0.000000      0.000000      1.000000   \n",
       "max    ...      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "             att158        att159        att160          week        Clicks  \\\n",
       "count  44945.000000  44945.000000  44945.000000  44945.000000  44945.000000   \n",
       "mean       0.003204      0.003538      0.003404     30.494271      8.055779   \n",
       "std        0.056513      0.059374      0.058246      0.499973     30.756230   \n",
       "min        0.000000      0.000000      0.000000     30.000000      1.000000   \n",
       "25%        0.000000      0.000000      0.000000     30.000000      1.000000   \n",
       "50%        0.000000      0.000000      0.000000     30.000000      2.000000   \n",
       "75%        0.000000      0.000000      0.000000     31.000000      5.000000   \n",
       "max        1.000000      1.000000      1.000000     31.000000   1801.000000   \n",
       "\n",
       "        Conversions  \n",
       "count  44945.000000  \n",
       "mean       0.467727  \n",
       "std        1.963123  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max       90.000000  \n",
       "\n",
       "[8 rows x 164 columns]"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Detection and Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we next take a high level look on some of the columns. This confirms our previous observation that all the attribute columns simply binary colums containing either 0 or 1 as a value, while the columns \"week\" contains either the value 30 or 31. Among the attribute columns we notice that some of them are highly imbalanced towards either 0 or 1.\n",
    "\n",
    "I also notice that the max values for the columns Conversions and Clicks are strangely high with 90 for Conversions and 1801 for Clicks. As these could be outliers, I will employ an Isolation Forest algorithm to partition the feature space and thus identify and eliminate outliers, which can possibly have a hugely positive effect on the model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolation_outlier(df, cols=None):\n",
    "    print('dataset with outliers:', len(df))\n",
    "    isof = IsolationForest(max_samples=1000, contamination=.075)\n",
    "    if cols == None:\n",
    "        isof.fit(df.select_dtypes(include=np.number))\n",
    "        listpred = isof.predict(df.select_dtypes(include=np.number))\n",
    "    else:\n",
    "        isof.fit(df[cols])\n",
    "        listpred = isof.predict(df[cols])\n",
    "    df['outlier'] = listpred\n",
    "    df = df.loc[df['outlier'] == 1]\n",
    "    df.drop('outlier', axis=1, inplace=True)\n",
    "    print('dataset without outliers:', len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset with outliers: 44945\n",
      "dataset without outliers: 41582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "df = isolation_outlier(df, cols=['Conversions', 'Clicks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 3500 rows were eliminated and as expected the column mean as well as the max value for both columns has decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conversions</th>\n",
       "      <th>Clicks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>41582.000000</td>\n",
       "      <td>41582.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.141071</td>\n",
       "      <td>3.651099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.419415</td>\n",
       "      <td>4.403579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Conversions        Clicks\n",
       "count  41582.000000  41582.000000\n",
       "mean       0.141071      3.651099\n",
       "std        0.419415      4.403579\n",
       "min        0.000000      1.000000\n",
       "25%        0.000000      1.000000\n",
       "50%        0.000000      2.000000\n",
       "75%        0.000000      4.000000\n",
       "max        3.000000     30.000000"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Conversions', 'Clicks']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['conversion_rate'] = df['Conversions'] / df['Clicks']\n",
    "df.drop(['Conversions', 'Clicks', 'entity_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're at it, let's also manually one hot encode the week feature, so that it is uniformly scaled with the attribute features, which might give us a slight boost in predictive power:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_30_binary = []\n",
    "for row in df['week'].iteritems():\n",
    "    if row[1] == 30:\n",
    "        week_30_binary.append(1)\n",
    "    else:\n",
    "        week_30_binary.append(0)\n",
    "\n",
    "df.drop('week', axis=1, inplace=True)\n",
    "df['is_week_30'] = week_30_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick check for NaN values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "att1          0\n",
       "att105        0\n",
       "att106        0\n",
       "att107        0\n",
       "att108        0\n",
       "             ..\n",
       "att57         0\n",
       "att58         0\n",
       "att59         0\n",
       "att41         0\n",
       "is_week_30    0\n",
       "Length: 163, dtype: int64"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operating under the assumption that our dataset is close to a perfect reprentation of business reality (which it definitely should be!), it is a valid method to stratify the sampling of our training and test datasets accordind to our target feature.\n",
    "\n",
    "Since our target feature is continous, though, stratifying it is not as straight forward as it would be with a binary one. So we basically create 225 bins (the number 225 was heuristically chosen) in which we 'place' instances of our target feature, so that within each bin the mean value of our target feature is equal. This we can then pass on to our \"stratify\" parameter, when splitting the data, so that train and test dataset have an equal distribution of the target feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, len(df),225)\n",
    "y_binned = np.digitize(df['conversion_rate'], bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('conversion_rate', axis=1), df['conversion_rate'], \n",
    "                                                    test_size=0.3, random_state=42, stratify=y_binned)\n",
    "\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our device column only has 3 values, so performing a simple one hot encoding will fully suffice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh = ce.OneHotEncoder(cols=['device'], drop_invariant=True, use_cat_names=True)\n",
    "oh.fit(X_train, y_train)\n",
    "\n",
    "X_train = oh.transform(X_train)\n",
    "X_test = oh.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization, K-Fold cross validation and Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are dealing with relatively high dimensional data, let's try using a linear algorithm for our regression model. Given the structure of our data, this stochastic gradient descent regression algorithm should perform well and also should not take long to fit compared to a non-parametric decision tree for example. We define a parameter space as well as MSE as our metric and pass this on to Bayesian Hyperparameter tuning instance with an in-build Cross Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_sgd = {\"loss\": ['squared_loss', 'huber', 'epsilon_insensitive','squared_epsilon_insensitive'],\n",
    "              \"penalty\": ['l1', 'l2', 'elasticnet'],\n",
    "              \"alpha\": [0.00001,0.0001, 0.001, 0.01, 0.1, 1],\n",
    "              \"max_iter\": [3000],\n",
    "              \"early_stopping\": [True]\n",
    "              }\n",
    "\n",
    "sgd = SGDRegressor()\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "bcv = BayesSearchCV(sgd, params_sgd,cv=10, n_iter=100, random_state=42, verbose=0, n_jobs=1, n_points=300, scoring=mse_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    #The Bayes Optimizer Library throws some pretty lenghty FutureWarnings during fitting. We will ignore them.\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    bcv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the hyperparameter optimization and the cross validation, we fit the the algorithm with the best parameters to the entire training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=1e-05, average=False, early_stopping=True, epsilon=0.1,\n",
       "             eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "             learning_rate='invscaling', loss='squared_loss', max_iter=3000,\n",
       "             n_iter_no_change=5, penalty='l1', power_t=0.25, random_state=None,\n",
       "             shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = bcv.best_estimator_\n",
    "best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction, Baselines and Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then predict on our test data. Also we create baseline predictions that either contain just zeroes, the mean of the entire dataset or the respective mean for each device.\n",
    "\n",
    "The results show that our algorithm has outperfomed the baselines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse sgd:  0.0219\n",
      "mse device:  0.0221\n",
      "mse mean:  0.0222\n",
      "mse zero:  0.0237\n"
     ]
    }
   ],
   "source": [
    "y_sgd_pred = best.predict(X_test)\n",
    "y_pred_mean = np.tile(df['conversion_rate'].mean(), len(y_sgd_pred))\n",
    "y_pred_zero = np.tile(np.array(0),len(y_sgd_pred))\n",
    "\n",
    "means = df_train.groupby('device')['conversion_rate'].mean()\n",
    "y_pred_device = []\n",
    "for row in df_test['device'].iteritems():\n",
    "    if row[1] == 'Computer':\n",
    "        y_pred_device.append(means[0])\n",
    "    elif row[1] == 'Smartphone':\n",
    "        y_pred_device.append(means[1])\n",
    "    elif row[1] == 'Tablet':\n",
    "        y_pred_device.append(means[2])\n",
    "    else:\n",
    "        print('Error')\n",
    "\n",
    "mse_zero = mean_squared_error(y_test, y_pred_zero)\n",
    "mse_mean = mean_squared_error(y_test, y_pred_mean)\n",
    "mse_device = mean_squared_error(y_test, y_pred_device)\n",
    "mse_sgd = mean_squared_error(y_test, y_sgd_pred)\n",
    "\n",
    "\n",
    "print('mse sgd: ', round(mse_sgd, 4))\n",
    "print('mse device: ', round(mse_device, 4))\n",
    "print('mse mean: ', round(mse_mean, 4))\n",
    "print('mse zero: ', round(mse_zero,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: Additional Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Dataset is quite high dimensional, which obviously comes with it's own problems such as limiting our choice of algorithms as well as making model training more computationally expensive. \n",
    "\n",
    "The attributes in our dataset are not further described in what they mean or represent. We only know that they are all binary and that some of them are highly imbalanced. This has made me think - \"Why not just add all attributes up within a row? Why not turn 160 columns into 1?\" - At best, this would reduce noise in the data and provide a better performing model, at worst the model would at least be much less computationally expensive due to much reduced dimensionality.\n",
    "\n",
    "So, this is what I did. I added up all attribute columns across each row into one single value, discarded the single attribute colums and retained the \"week\" and \"device columns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_att = X_train.drop(['device_Computer', 'device_Smartphone','device_Tablet','is_week_30'], axis=1)\n",
    "X_train_else = X_train[['device_Computer', 'device_Smartphone','device_Tablet', 'is_week_30']]\n",
    "X_train_att_sum = X_train_att.sum(axis=1)\n",
    "\n",
    "X_train_sum_full = pd.concat([X_train_att_sum, X_train_else], axis=1)\n",
    "X_train_sum_full.shape\n",
    "\n",
    "\n",
    "X_test_att = X_test.drop(['device_Computer', 'device_Smartphone','device_Tablet','is_week_30'], axis=1)\n",
    "X_test_else = X_test[['device_Computer', 'device_Smartphone','device_Tablet', 'is_week_30']]\n",
    "X_test_att_sum = X_test_att.sum(axis=1)\n",
    "\n",
    "X_test_sum_full = pd.concat([X_test_att_sum, X_test_else], axis=1)\n",
    "X_test_sum_full.shape\n",
    "\n",
    "X_train_sum_full.rename(columns={0: \"sum\"}, inplace=True)\n",
    "X_test_sum_full.rename(columns={0: \"sum\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have excluded a tree based algorithm due to the relatively high dimensionality of our data. Now that this is greatly reduced, why not give our good old friend XGBoost a shot? \n",
    "\n",
    "As previously done, we have parameter space as well as Bayesian Hyperparameter Optimization with a K-Fold Cross Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_xgr = {\"n_estimators\": [10,20,50,100],\n",
    "              \"max_depth\": list(range(1, 11)),\n",
    "              \"learning_rate\": [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "              \"subsample\": list(np.arange(0.05, 1.01, 0.05)),\n",
    "              \"min_child_weight\": list(range(1, 21)),\n",
    "              \"colsample_bytree\": list(np.arange(0.1,1,0.1)),\n",
    "              \"reg_lambda\": [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.],\n",
    "              \"reg_alpha\": [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.]\n",
    "\n",
    "              }\n",
    "xgr = XGBRegressor()\n",
    "bcvxg = BayesSearchCV(xgr, params_xgr,cv=10, n_iter=100, random_state=42, verbose=0, n_jobs=1, n_points=300, \n",
    "                      scoring=mse_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    #The Bayes Optimizer Library throws some pretty lenghty FutureWarnings during fitting. We will ignore them.\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    bcvxg.fit(X_train_sum_full, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again train on the whole training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "best2 = bcvxg.best_estimator_\n",
    "best2.fit(X_train_sum_full, y_train)\n",
    " \n",
    "mse_xg = mean_squared_error(y_test, best2.predict(X_test_sum_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that our XGBoost model actually performed almost as good as our SGD model, which was trained on the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse sgd:  0.0219\n",
      "mse xg summed attributes:  0.0221\n",
      "mse device:  0.0221\n",
      "mse mean:  0.0222\n",
      "mse zero:  0.0237\n"
     ]
    }
   ],
   "source": [
    "print('mse sgd: ', round(mse_sgd, 4))\n",
    "print('mse xg summed attributes: ', round(mse_xg, 4))\n",
    "print('mse device: ', round(mse_device, 4))\n",
    "print('mse mean: ',round(mse_mean, 4))\n",
    "print('mse zero: ', round(mse_zero, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Data Set and Export of Validation Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv('test_w_weeks.csv')\n",
    "val_trans = val.drop('entity_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_trans = oh.transform(val_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions_sgd_full_data = best.predict(val_trans)\n",
    "val['predictions'] = val_predictions_sgd_full_data\n",
    "val_final = val[['entity_id', 'device', 'predictions']].sort_values(by=['entity_id', 'device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_final.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Achievements\n",
    "\n",
    "In this Notebook, I was able to:\n",
    "\n",
    "- Create a ML model that outperformed Naive baselines\n",
    "- Transform data with the benefit of reducing computional cost and changing the data structure to make the data more widely usable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
